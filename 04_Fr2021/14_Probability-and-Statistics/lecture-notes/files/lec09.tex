An interesting application of the Borel-Cantelli lemma is the following ``experiment''
\begin{ex}[]
Let $N \in \N$ and $\{x_1,\ldots,x_n\}$ be a ``binary text'' of length $N$. Then the probability that the text appears in any outcome is $1$.

To show this, we can consider the events $A_k$ for $k = 1, 2, \ldots$ given by
\begin{align*}
  A_k := \{X_{(k-1)N} = x_1, \ldots X_{kN} = x_N\}
\end{align*}
these are all independent and have non-zero probability $\IP[A_k] > 0$. 
Then we can apply the Borel-Cantelli Lemma.
\end{ex}

\subsection{Transformation of P-spaces}
In the following, let $(\Omega,\mathcal{A},\IP)$ be a $P$-space, $\tilde{\Omega} \neq \emptyset$, and $\tilde{\mathcal{A}} \subseteq \mathcal{P}(\tilde{\Omega})$ a $\sigma$-Algebra on $\tilde{\Omega}$.

Given a map $\phi: \Omega \to \tilde{\Omega}$, we wish to induce a probability measure on $\tilde{\Omega}$ respecting $\phi$. 
But that is not always possible for every $\phi$.
\begin{dfn}[]
  A map $\phi: \Omega \to \tilde{\Omega}$ is \textbf{measurable} (with respect to $\mathcal{A}$ and $\tilde{\mathcal{A}}$) if $\phi^{-1}(\tilde{\mathcal{A}}) \subseteq \mathcal{A}$, i.e.
  \begin{align*}
    \phi^{-1}(\tilde{A}) \in \mathcal{A}, \quad \text{for all} \quad \tilde{A} \in \mathcal{A}
  \end{align*}
\end{dfn}
Note that if $\tilde{\mathcal{A}}$ is the $\sigma$-Algebra generated by a collection of subsets $\tilde{\mathcal{A}}_0 \subseteq \tilde{\mathcal{A}}$, then it is sufficient to check if
\begin{align*}
  \phi^{-1}(\tilde{A}) \in \mathcal{A}, \quad \text{for all} \quad \tilde{A} \in \tilde{\mathcal{A}}_0
\end{align*}
because the collection $\{\tilde{A} \subseteq \tilde{\Omega} \big\vert \phi^{-1}(\tilde{A}) \in \mathcal{A}\}$ is a $\sigma$-Algebra containing $\tilde{\mathcal{A}}_0$.



\begin{prop}[]
If $\phi: \Omega \to \tilde{\Omega}$ is measuarble, then we can obtain a probability measure $\tilde{\IP}$ on $\tilde{\mathcal{A}}$ given by
\begin{align*}
  \tilde{\IP}[\tilde{A}] := \IP\left[\phi^{-1}(\tilde{A})\right] \quad \text{for all} \quad \tilde{A} \in \tilde{\mathcal{A}}
\end{align*}
we call $\tilde{\IP}$ the image of $\IP$ under $\phi$ and write $\tilde{\IP} = \IP \circ \phi^{-1}$.
\end{prop}

Using the notion of measurable maps, we can re-define what a random variable is and obtaina nicer definition of a distribution of a random variable.
\begin{dfn}[]
  Let $(\Omega,\mathcal{A},\IP)$ be a $P$-space. A \textbf{random variable} is a measurable map
  \begin{align*}
    X: (\Omega,\mathcal{A}) \to (\R,\mathcal{B})
  \end{align*}
  , where $\mathcal{B}$ is the Borel $\sigma$-Algebra on $\R$ generated by intervals of the Form $(- \infty, b]$, for $b \in \R$, which contains all open and closed sets.

  The \textbf{distribution} $\mu$ of $X$ is the pushforward $\IP \circ X^{-1}$ given by
  \begin{align*}
    \mu(A) = \IP[X^{-1}(A)] = \IP[\{\omega \in \Omega: X(\omega) \in A\}] \quad \text{for} \quad A \in \mathcal{B}
  \end{align*}
\end{dfn}

\begin{ex}[]
  In the case where $(\Omega,\mathcal{F},\IP)$ is the model of $0-1$ experiments and $X$ is the time until the first $1$:
  \begin{align*}
    X(\omega) = \min \{k \geq 1: x_k = 1\}
  \end{align*}
  then the distribution of $\mu$ is given by
  \begin{align*}
    \mu(\{k\}) = \IP[X = k] = \IP[]\{\omega \in \Omega: x_1 = \ldots = x_{k-1} = 0, x_k = 1\} = (1-p)^{k-1}p
  \end{align*}
  for singletons, and for arbitrary $A \in \mathcal{B}$, it is
  \begin{align*}
    \mu(A) = \sum_{k \in A}(1-p)^{k-1}
  \end{align*}
  which gives rise to the \textbf{geometric} distribution on $\N$, as for $p = \tfrac{1}{2}$ we obtain the geometric series.
\end{ex}

\begin{dfn}[]
  Let $X$ be a random variable on $(\Omega,\mathcal{A},\IP)$. 
  The function 
  \begin{align*}
    F: \R \to [0,1], \quad F(b) := \IP[X \leq b] = \mu((-\infty,b])
  \end{align*}
  is called the \textbf{distribution function} of $X$ (or $\mu$).
\end{dfn}

\begin{rem}[]
First note the following. For $a < b \in \R$ we have
\begin{align*}
  \mu((a,b]) = F(b) - F(a)
\end{align*}
and we can obtain the discontinuity of $F$ at a point $a$ by evaluating $\mu$ at $a$:
\begin{align*}
  \mu(\{a\}) 
  &= \lim_{n \to \infty} \left(
    \left(
      a - \frac{1}{n}, a
    \right]
  \right)
  \\
  &= F(a) - \lim_{h > 0 \to 0} F(a - h)
\end{align*}
\end{rem}

\begin{thm}[]
This distribution function has the following properties
\begin{enumerate}
  \item Monotoneity: $a \leq b \implies F(a) \leq F(b)$
  \item \texttt{cadlag}\footnote{Continue à droit, limite à gauche}: $F(a) = \lim_{h> 0 \to 0} F(a + h)$
  \item $\lim_{a \to -\infty}F(a) = 0$ and $\lim_{a \to \infty}F(a) = 1$
\end{enumerate}
On the other hand, every function with these three properties is the distribution function of a random variable $X$ (we sometimes write $F^{-1}$).
We call $X$ the \textbf{quantile} of the distribution, or more explicitly, we say $X(t)$ the $t$-quantile of $F$.

An important example is the $50\%$-Quantile $X(\tfrac{1}{2})$, also known as the \textbf{median}.
\end{thm}
The properties are easy to check. 
To prove the existence of such a random variable, we require the following lemma
\begin{lem}[]
  Let $F$ be such that it satisfies the properties (a) - (c) and define 
  \begin{align*}
    X(t) = \inf \{x \big\vert F(x) \geq t\}
  \end{align*}
  then $X$ is monotonous, left-continuous and 
  \begin{align*}
    X(F(x)) \leq x\quad \forall x \in \R \quad \text{and} \quad t \leq F(X(t)) \quad\forall t \in (0,1)
  \end{align*}
\end{lem}
\begin{proof}[Proof Lemma]

\end{proof}
\begin{proof}[Proof Theorem]
  If $F$ satisfies these properties, then for $0 < t < 1$ we define
  \begin{align*}
    X(t) := \inf \{x \big\vert F(x) \geq t\}
  \end{align*}
  This function is measurable and by the lemma, we we have
  \begin{align*}
    X(t) \leq x \iff t \leq F(x)
  \end{align*}
  Then we chose the equal distribution $\IP$ on $[0,1]$ and so we get
  \begin{align*}
    \IP[X \leq b] = \IP\left[\{\omega: X(\omega) \leq b\}\right] = \IP\left[\{\omega: \omega \leq F(b)\}\right] = F(b)
  \end{align*}
\end{proof}

