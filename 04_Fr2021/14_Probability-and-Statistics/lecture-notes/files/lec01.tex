\section{Probability theory}
\subsection{Introduction}

Although even the greeks and 17-th century mathematicians 
a mathematical framework to discuss probability and statistics was only developed in the 19-th and 20-th century.

One reason this might have been the case could be the empirical nature of statistics. 
Probabily theory was dependent on data and was difficult to axiomatize and whose proofs often relied on human intuition over formal derivation.

One difficulty was that in order to find a suitable notion of probability on an infinite dimensional vector space.
For example, to think about the probability that a particle takes any given Brownian motion path is to find a notion of measure on the set $C([0,\infty), \R)$.

As a consequence, the first fields medal that was awarded for a contribution to probability was in 2002 and 2006. Almost 70 years after the first medal was awarded.

So the axiomatisation was only done in the early 20-eth century and reached the center of mathematics in the 21-st century.


\subsection{Probability Space}

\begin{dfn}[Probability space]
  A \textbf{Probability space} (or P-space) is a triple $(\Omega,\mathcal{A}, \IP)$ consisting of a non-empty set $\Omega$, a $\sigma$-Algebra $\mathcal{A} \subseteq 2^{\Omega}$. And a so called $\sigma$-additive probability measure $\IP: \mathcal{A} \to [0,1]$
  That is,
  \begin{enumerate}
    \item $\Omega \in \mathcal{A}$
    \item $\forall A \in \mathcal{A}: A^{c} \in \mathcal{A}$
    \item $\forall (A_i)_{i \in \N}, A_i \in \mathcal{A}$, $\bigcup_{i \in \N} A_i \in \mathcal{A}$
    \item $\IP[\es] = 0, \IP[\Omega] = 1$
    \item $\forall (A_i)_{i \in \N}, A_i \in \mathcal{A}$, if all $A_i$ are disjoint, then
      $\IP\left(\bigsqcup_{i \in \N} A_i\right) = \sum_{i \in \N} \IP[A_i]$
  \end{enumerate}
\end{dfn}

It is useful (and sometimes necessary) to be able to talk about the ``smallest measureable sets''.
\begin{dfn}[]
A subset $A \in \mathcal{A}$ of $\Omega$ is \textbf{atomic}, if 
\begin{align*}
  B \in \mathcal{A}, B \subseteq A \implies B = \emptyset \lor B = A
\end{align*}
We denote the set of all atoms as $\text{Atom}(\mathcal{A})$. 
From this, we can uniquely decompose any element $B \in \mathcal{A}$ into a disjoint union of atoms.
\end{dfn}

\begin{rem}[]
  Often, it is useful to set $\mathcal{A} = \mathcal{P}(\Omega)$ i.e to let all subsets of $\Omega$ be measureable. In particular, the atoms are then exactly the singletons $\omega \in \Omega$. If that is the case, we can define a \textbf{weight} function
\begin{align*}
  p: \Omega \to [0,1], \quad p(\omega) := \IP[\{\omega\}]
\end{align*}
  Then for any subset $A \subseteq \Omega$, its probability measure can be calculated as follows
  \begin{align*}
    \IP[A] = \sum_{\omega \in A}p(\omega)
  \end{align*}
  or in the general case, for $B \in \mathcal{A} \neq \mathcal{P}(\Omega)$:
  \begin{align*}
    \IP[B] = \sum_{A \in \text{Atom}(\mathcal{A})} \IP[A \cap B]
  \end{align*}
\end{rem}

It can easily shown that the following axioms are satified:
\begin{enumerate}
  \item $\IP[A^{c}] = 1 - \IP[A]$
  \item $\IP[A\cup B] = \IP[A] + \IP[B] - \IP[A \cap B]$
  \item $\IP\left(\bigcup_{i = 1}^n A_i\right) = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 <\ldots < i_k \leq n} \IP[A_{i_1} \cap \ldots \cap A_{i_k}]$
  \item $A \subseteq B \implies \IP[A] \leq \IP[B]$
\end{enumerate}
where c) can be derived using an inductive generalisation of b).

\begin{ex}[Bernoulli-Experiment]
  Consider the experiment where we measure the number of calls during a given time at a call center. So $\Omega = \{0,1,2,\ldots\}$. 
  We can set the weights to be 
  \begin{align*}
    p(\omega) = e^{-\lambda} \frac{\lambda^{\omega}}{\omega!}, \quad \text{for} \quad \omega \in \Omega
  \end{align*}
  , where $\lambda > 0$ is some parameter. This model is called the \textbf{Poisson-Distribution}.
  Note that the above defintiion is well defined, as
  \begin{align*}
    \IP[\Omega] = \sum_{n \in \N} e^{-\lambda} \frac{\lambda^{n}}{n!} = e^{- \lambda} \cdot e^{\lambda} = 1
  \end{align*}
  If we let $A = \{1,2, \ldots\}$ to be the outcome, where we get at least one call, then we see that
  \begin{align*}
    \IP[A] = 1 - \IP[A^{c}] = 1 - \IP[\{0\}) = 1 - e^{- \lambda}
  \end{align*}
\end{ex}

What is often very useful is to associate to each possible outcome $\omega \in \Omega$ a real value. This gives us the following definition.

\begin{dfn}[Random Variable]
  A \textbf{random variable} is a real-valued function $X: \Omega \to \R$.


  If the image $X(\Omega)$ is also countable, then we can turn the probability measure $\IP$ on $\Omega$ into a probability measure on the image $X(\Omega)$ where for $x \in X(\Omega)$ we define
  \begin{align*}
    \IP[X = x] := \IP\left[\{\omega\in \Omega \big\vert X(\omega) = x\}\right] = \IP[X^{-1}(x)]
  \end{align*}
  which reads as: ``The probability that the random variable $X$ obtains the value $x \in X(\Omega)$ is the probability measure of the preimage of $x$ under the function $X$.''

  In the case for general $\mathcal{A}$, we must require that $X$ be constant on every atom, or equivalently, that the preimage of singletons under $X$ are $\mathcal{A}$-measureable. 
  \begin{align*}
    \IP[X = x] := \IP \left[
      \{A \in \text{Atom}(\Omega) \big\vert X(A) = x\}
    \right]
    = \IP[X^{-1}(x)]
  \end{align*}
  Other common notation for this is $\mu_X(x), P(X=x), P(x), P_x$ etc.
\end{dfn}
Given a P-space $(\Omega,\mathcal{A},P)$ we can restrict the probability measure $\IP: \mathcal{A} \to [0,1]$ to a subset $\mathcal{B} \subseteq \mathcal{A}$ to obtain a new P-space $(\Omega, \mathcal{B}, \IP_{\mathcal{B}})$.

Note that a random variable $X$ with respect to the $\sigma$-Algebra $\mathcal{A}$ need not be a random variable with respect to $\mathcal{B}$, as the preimages might not be measureable anymore.

Say we take a random variable $X$ and chose a random element (or Atom) of $\Omega$. What would be the best guess on the value that $X$ takes on $\omega$?

\begin{dfn}
  For a random variable $X: \Omega \to \R$ we define the \textbf{expectation value} of $X$ to be
  \begin{align*}
    \E[X] := \sum_{\omega \in \Omega}X(\omega) p(\omega) \quad \left(\text{or} \quad \E[X] := \sum_{A \in \text{Atom}(\Omega)} X(A)\IP[A]
    \right)
  \end{align*}
  where the right hand side can be problematic if $X$ can take on negative values as the non-absolute converge may occur.
  To remedy this, we can divide $X$ into its positive and negative parts 
  \begin{align*}
    X(\omega) = X^+(\omega) - X^-(\omega) = \max\{X(\omega), 0\} + \{\min\{0, X(\omega)\}
  \end{align*}
  and then change the definition of the expectation as follows:
  \begin{align*}
    \E[X] := \E[X^+] - \E[X^-]= \sum_{X(\omega) > 0}X(\omega) p(\omega) - \sum_{X(\omega) < 0}\underbrace{-X(\omega)p(\omega)}_{\geq 0}
  \end{align*}
  if not both sums are infinite.
\end{dfn}
The expectation value can also be expressed in terms of the distribution of $X$:
\begin{align*}
  \E[X] = \sum_{x \in X(\Omega)} \sum_{\omega: X(\omega) = x} X(\omega) p(\omega) =
  \sum_{x \in X(\Omega)} x \cdot \IP[X = x]
\end{align*}

\begin{ex}[]
  In the call-center example from before, let $X$ be the number of calls, so $X(\omega) = \omega$. Then
  \begin{align*}
    \E[X] = \sum_{k \in \N} k \IP[X = k] = \sum_{k=0}^{\infty}k e^{-\lambda} \frac{\lambda^{k}}{k!} = \lambda \sum_{k=1}^{\infty}e^{-\lambda} \frac{l^{k-1}}{(k-1)!} = \lambda
  \end{align*}
  so the paramter $\lambda$ gives us the expected number of calls.
\end{ex}

\begin{ex}[]
  In an insurance contract the cost to the insurance company is
  \begin{align*}
    X = \left\{\begin{array}{ll}
      c & \text{if the event $A$ occurs} \\
      0 & \text{else}
    \end{array} \right.
  \end{align*}
  The premiums the insured have to pay should then be the expectation value
  \begin{align*}
    \E[X] = c \cdot \IP[A] + 0 \cdot \IP[A^{c}] = c \cdot \IP[A]
  \end{align*}
\end{ex}
More generally, we can define the \textbf{indicator function}
\begin{align*}
  I_B(\omega) := \left\{\begin{array}{ll}
      1 & \text{for } \omega \in B \\
     0 & \text{for } \omega \notin B
  \end{array} \right.
\end{align*}
  for subsets $B \subseteq \Omega$, then we can write $X = c I_A$, so the expectation value is
  \begin{align*}
    \E[cI_B] = c \IP[B] \quad \text{for} \quad c \in \R, B \in \mathcal{A}
  \end{align*}
This immediately gives us the following lemma
\begin{lem}[Linearity of the expectation value]
  It follows directly from the definition that the expectation value is linear in the sense that, if $X,Y: \Omega \to \R$ are random variables and $a,b \in \R$, then
  \begin{align*}
    \E[aX + bY] = a\E[X] + bE[Y]
  \end{align*}
\end{lem}

Another useful result when calculating the expectation value is
\begin{lem}[]
If $X$ only takes values in $\N$, then
\begin{align*}
  \E[X]= \sum_{j \in \N} \IP[X > j]
\end{align*}
\end{lem}
\begin{proof}
  We can use the representation using the distribution to find
  \begin{align*}
    \E[X] &= \sum_{k=1}^{\infty} k \IP[X = k]\\
    &= \sum_{k=1}^{\infty}\sum_{j=0}^{k-1}\IP[X = k]\\
    &= \sum_{j=0}^{\infty}\sum_{k=j+1}^{\infty}\IP[X=k]\\
    &=\sum_{j=0}^{\infty} \IP[X > j]
  \end{align*}
\end{proof}

