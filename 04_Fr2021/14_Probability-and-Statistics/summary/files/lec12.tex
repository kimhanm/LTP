Just like in the $1$-dimensional case: 
If every $X_i$ is discrete, then the image $\bm{X}(\Omega)$ is countable and we define
\begin{align*}
  \mu_{\bm{X}}(A) = \sum_{\bm{x} \in \bm{X}(\Omega) \cap A} \IP[\bm{X} = \bm{x}] = \sum_{\underset{x_i \in X_i(\Omega)}{(x_{1}, \ldots, x_{n}) \in A}} \IP[X_i = x_i]_{i \in I}
\end{align*}
If the collective distribution \textbf{absolutely continuous}, i.e. if there exists a measurable function $f: \R^{n} \to \R_{\geq 0}$ such that
\begin{align*}
  \mu_{\bm{X}}(A) = \int_A f(\bm{x}) d \bm{x}
\end{align*}
and call this function $f$ the \textbf{density function} of $\bm{X}$.

Given the collective distribution, we can obtain a distribution for a single $X_i$. 
The \textbf{marginal distribution} is the distribution obtained by 
\begin{align*}
  \mu_{X_i}(B) = \IP[X_i \in B] = \mu_{\bm{X}}(\R \times \ldots \times B \times \ldots \times\R) \quad \text{for } B \in \mathcal{B}
\end{align*}

Note that the marginal distribution does not uniquely determine the collective distribution.
The missing information is the (in-)dependence of the random variables of the components of $\bm{X}$.

\begin{dfn}[]
  The random variables $X_{1}, \ldots, X_{n}$ are (stochastically) \textbf{indepenent}, if for all $A_1, \ldots, A_n \in \mathcal{B}$
  \begin{align*}
    \IP[X_1 \in A_1, \ldots, X_n \in A_n] = \prod_{i=1}^{n}\IP[X_i \in A_i]
  \end{align*}
  or equivalently
  \begin{align*}
    \mu_{\bm{X}}\left(\prod_{i=1}^{n}A_i\right) = \prod_{i=1}^{n} \mu_{X_i}(A_{i})
  \end{align*}
\end{dfn}


\begin{ex}[]
  We define the \textbf{standard normal distribution} for independent random variables $X_{1}, \ldots, X_{n}$ using the $N(0,1)$ distribution.
  The collective distribution then has density
  \begin{align*}
    f(\bm{x}) = (2 \pi)^{-n/2} \exp \left(
      - \frac{1}{2} \sum_{i=1}^{n} x_i^{2}
    \right)
    = (2 \pi)^{-n/2} e^{- \tfrac{1}{2}\abs{\bm{x}}^{2}}
  \end{align*}
\end{ex}

Analogously to the one-dimensional case we can define the push-forwards for a random variable along a \textbf{measurable map} $g: (\R^{n},\mathcal{B}^{n}) \to (\R^{m},\mathcal{B}^{m})$ as
\begin{align*}
  \mu_{\bm{Y}}(A) = \mu_{g \circ \bm{X}} := \mu_{\bm{X}}(g^{-1}(A)) \quad\text{ for } A \in \mathcal{B}^{m}
\end{align*}

\begin{prop}[]
  Let $g: \R^{n} \to \R^{m}$ be linear and invertible: $g(\bm{x}) = \bm{m} + B \bm{x}$ with $\det B \neq 0$.
  If $\mu_{\bm{x}}$ is absolutely continuous, then so is $\mu_{\bm{Y}}$ and its density is given by
  \begin{align*}
    f_{\bm{Y}}(\bm{y}) = \frac{1}{\abs{\det B}} f_{\bm{X}}\left(
      B^{-1}(\bm{y} - \bm{m})
    \right)
  \end{align*}
\end{prop}
\begin{proof}
  This follows fromthe subtitution rule with $\bm{x} = B^{-1}(\bm{y} - \bm{m})$ as
  \begin{align*}
    \mu_{\bm{Y}}(A) = \mu_{\bm{X}}(g^{-1}(A)) = \int_{g^{-1}(A)}f_{\bm{X}}(\bm{x}) d \bm{x} = \int_A f_{\bm{X}}(g^{-1}(\bm{y})) \frac{1}{\abs{\det B}} d \bm{y}
  \end{align*}
\end{proof}

\begin{ex}[]
An important example is the push-forward of the $n$-dimensional standard normal distribution with the substitution $\bm{Y} = \bm{m} + B \bm{X}$.
It has density
\begin{align*}
  f_{\bm{Y}}(\bm{y}) = (2 \pi)^{-n/2} \frac{1}{\sqrt{\abs{\det \Sigma}}} \exp\left(
    - \frac{1}{2} (\bm{y} - \bm{m})^{T} \Sigma^{-1}(\bm{y} - \bm{m})
  \right)
\end{align*}
where $\Sigma = BB^{T}$.
This gives us is the generalized $n$-dimensional Nromal distribution $N_n(\bm{m},\Sigma)$.
\end{ex}

\subsection{Covariance and Correlation}
Let $g: (\R^{n}, \mathcal{B}^{n}) \to (\R,\mathcal{B})$ be measureable. 
Instead of computing $\E[g \circ X]$ directly, we just use
\begin{align*}
  \E[g \circ \bm{X}] = \int_{\R^{n}} g(\bm{x}) \mu_X(d \bm{x})
\end{align*}
which in the discrete case is
\begin{align*}
  \E[g \circ \bm{X}] = \sum_{x_i \in X_i(\omega)}g(x_{1}, \ldots, x_{n}) \IP[X_i = x_i]_{i \in I}
\end{align*}
or in the absolutely continuous case
\begin{align*}
  \E[g \circ \bm{X}] = \int_{\R^{n}}g(\bm{x})f_{\bm{X}}(\bm{x}) d \bm{x}
\end{align*}

We can use this to define
\begin{dfn}[]
The \textbf{Covariance} of random variables $X_1,X_2$ is defined as
\begin{empheq}[box=\bluebase]{align*}
  \Cov(X_1,X_2) := \E \left[
    (X_1 - \E[X_1]) (X_2 - \E[X_2]) 
  \right]
\end{empheq}
\end{dfn} 
\begin{prop}[]
The covariance fulfills the following relations:
\begin{enumerate}
  \item $\Cov(X,X) = \variance[X]$
  \item $\Cov(X_1,X_2) = \Cov(X_2,X_1)$
  \item $\Cov(X_1,X_2) = \E[X_1 X_2] - \E[X_1]\E[X_2]$
  \item $\Cov(X_1,aX_2 + b) = a \Cov(X_1,X_2)$
  \item $\Cov(X_1,X_2 + X_3) = \Cov(X_1,X_2) + \Cov(X_1,X_3)$
  \item $\variance[X_1 + X_2] = \variance[X_1] + \variance[X_2] + 2 \Cov(X_1,X_2)$
  \item $\abs{\Cov(X_1,X_2)} \leq \sigma(X_1) \sigma(X_2)$
  \item If $X_1,X_2$ are independent, then $\Cov(X_1,X_2) = 0$. In particular we then have $\variance[X_1 + X_2] = \variance[X_1] + \variance[X_2]$.
\end{enumerate}
\end{prop}
Note that the converse of (h) is wrong. If we set $X_1$ corresponding to the normal distribution $N(0,1)$ and $X_2 = X_1^{2}$, then
\begin{align*}
  \Cov(X_1,X_2) = \E\left[(X_1 - \E[X_1])(X_1^{2} - \E[X_2]\right] = \E[X_1X_2] = \E[X_1^{3}] = 0
\end{align*}


\begin{dfn}[]
Given two random Variables $X_1,X_2$ the \textbf{correlation} between them is
\begin{empheq}[box=\bluebase]{align*}
  \rho(X_1,X_2) := \frac{\Cov(X_1,X_2)}{\sigma(X_1)\sigma(X_2)}
\end{empheq}
if $\rho(X_1,X_2) = 0$, we say that $X_1,X_2$ are uncorrelated.
\end{dfn}
Correlation measures strength and direction of \emph{linear dependence} between random variables.


It's 2021 and the public media is no stranger to the words \emph{correlation}, \emph{standard deviation}, \emph{statistics}, \emph{median} etc.
Unfortunately, the numbers are often misused to support statements that aren't actually supported by the data and it is our responsibility to be precise in our usage of statistics.

We often have to ask ourselves what the underlying P-space is to understand what these words mean.
Depending on how the P-space is chosen, one can come to wildly idfferent conclusions with the same raw data.
