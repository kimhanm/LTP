\section{The Galois Correspondence}

One might ask if the inverse of Theorem \ref{thm:3-14} is true, that is: 
If $E/k$ is a normal extension for a polynomial $f \in k[X]$ and $\Gal(E/k)$ is resolvable, does that mean that $f$ is solvable by radicals?

More generally one might ask if there is a correspondence between subgroups of $\Gal(E/k)$ and sandwiched fields $k \subseteq B \subseteq E$.

It turns out that such a correspondence exists if we impose the property that $f$ be separable.
We wish to better understand this correspondence in this chapter.

Looking at the definition of the galois group of an extension $E/k$, it consists of automorphisms that keep the field $k$ fixed.
But what can happen is that the Galois group fixes more points than just $k$ and we want to find out when that is the case.

\begin{dfn}[]
  Let $E$ be a field and $H \subseteq \Aut E$ a subset. The set
  \begin{align*}
    E^{h} := \{a \in E \big\vert \sigma(a) = a \forall \sigma \in H\}
  \end{align*}
  is a subfield of $E$ called the \textbf{fixing field} of $H$.
\end{dfn}
Note that the map $H \mapsto E^{H}$ is contravariant with respect to inclusion
\begin{align*}
  H_1 \subseteq H_2 \implies E^{H_2} \subseteq E^{H_1}
\end{align*}
and if $\scal{H}$ is the subgroup generated by $H$, $E^{<H>} = E^{H}$.

If $H$ is the galois group of a field extension $E/k$, then trivially $k \subseteq E^{\Gal(E/k)}$.

\begin{ex}[]
  Take for example $k = \F_p(t), f(X) = X^{p}-t$ and $E$ a splitting field of $f$.
  From the $X^{p}-c$ Lemma, we know that $\Gal(E/k) = \{e\}$, which shows $k \subsetneq E^{\Gal(E/k)} = E$.
\end{ex}

If $H < \Aut E$ is a finite subgroup, we can find out the order of the extension $E/E^{H}$.


In MMP-II, we saw that we can understand a group by studying its character table, where the characters are group homomorphisms of a group $G$ into the circle group $\IS^{1}$. 
Here we provide a more generalized definition
\begin{dfn}[]
  A \textbf{character} of a group $G$ in a field $E$ is a group homomorphism $\chi: G \to  E^{\times}$, into the multiplicative group of $E$.

  We denote the set of characters with $\Hom_{\Grp}(G,E^{\times})$.
  And we write
  \begin{align*}
    F(G,E) = \{\phi: G \to E\}
  \end{align*}
  for the $E$-vector space of $E$-valued functions on $G$.
\end{dfn}


Also from MMP-II, we know that the characters in $\Hom_{\Grp}(G,\IS^{1})$ fulfill the orthogonality relation.
For arbitrary fields $E$, we can get a weaker version:

\begin{prop}[Dedekind]\label{prop:dedekind}
  $\Hom_{\Grp}(G,E^{\times}) \subseteq F(G,E)$ is linearly independent.
\end{prop}
\begin{proof}
  If assume the opposite, then let $n \geq 2$ be the minimal number such that there exists characters $\chi_1, \ldots, \chi_n$ that are linarly dependend in $F(G,E)$, so
  \begin{align*}
    \exists c_1, \ldots, c_n \in E \setminus \{0\} \text{ with } c_1 \chi_1(x) + \ldots + c_n \chi_n(x) = 0 \quad \forall x \in G
  \end{align*}
  Since $\chi_1 \neq \chi_2$, there exists a $y \in G$ with $\chi_1(y) \neq \chi_2(y)$.
  By evaluating the above expressionat $x \cdot y$, we get
  \begin{align*}
    c_1 \chi_1(x) \chi_1(y) + \ldots + c_n \chi_n(x) \chi_n(y) = 0 \quad \forall  x \in G
  \end{align*}
  and after dividing by $\chi_n(y)$ and subtracting the original equation, we get
  \begin{align*}
    c_1 \left(
      \frac{\chi_1(y)}{\chi_n(y)} - 1
    \right)
    \chi_1(x)
    +
    c_2 \left(
      \frac{\chi_2(y)}{\chi_n(y)} - 1
    \right)
    \chi_2(x)
    + \ldots
    +
    c_{n-1} \left(
      \frac{\chi_{n-1}(y)}{\chi_n(y)} - 1
    \right)
    \chi_{n-1}(x)
    =
    0
  \end{align*}
  but in particular $\frac{\chi_1(y)}{\chi_n(y)} - 1 \neq 0$, which shows that the $\chi_{1}, \ldots, \chi_{n-1}$ are linearly dependent, in contradiction to the minimality of $n$.
\end{proof}

We will use this proposition with another lemma to find a lower bound for $[E:E^{G}]$. 


\begin{lemma}[]\label{lem:4-6}
  Let $E$ be a field and $S$ a set with $\sigma_{1}, \ldots, \sigma_{n} \in F(S,E)$ linearly independent.

  Then there exist $s_{1}, \ldots, s_{n} \in S$ such that the vectors
  \begin{align*}
    \begin{pmatrix}
      \sigma_1(s_1)\\
    \vdots\\
    \sigma_n(s_1)
    \end{pmatrix}
    , \ldots
    ,
    \begin{pmatrix}
      \sigma_1(s_n)\\
    \vdots\\
    \sigma_n(s_n)
    \end{pmatrix}
  \end{align*}
  are linearly independent in $E^{n}$.
\end{lemma}
The proof of this is done in the exercise sheet 8.


\begin{lem}[]\label{lem:4-7}
  Let $H = \{\sigma_1, \ldots, \sigma_n\} \subseteq \Aut E$. Then $[E:E^{H}] \geq n$.
\end{lem}
\begin{proof}
  The maps $\sigma_1|_{E^{\times}}, \ldots, \sigma_n|_{E^{\times}}$ are $n$ characters and thus linearly independent in $F(G,E)$
  By the sublemma, there exist scalaer $\{y_{1}, \ldots, y_{n}\} \subseteq E^{\times}$ such that the vectors
  \begin{align*}
    (\sigma_1(y_1), \ldots \sigma_n(y_1)), \ldots , (\sigma_1(y_n), \ldots, \sigma_n(y_n))
  \end{align*}
  are linearly independent.
  If we view $E$ as a vector space over the subfield $E^{H}$, then we can show that $\{y_{1}, \ldots, y_{n}\}$ are linearly independnent over $E^{H}$, which shows the proof.

  Indeed, if we had scalars $c_{1}, \ldots, c_{n} \in E^{H}$ that give a linear combination of $y_i$ summing to zero, then these would also give
  \begin{align*}
    \sum_{i=1}^{n}c_i \sigma_j(y_i) = 0 \quad \forall j = 1, \ldots, n
  \end{align*}
  but we already know that the vectors above are linearly independent, we would have that the corresponding matrix has full rank.
  So
  \begin{align*}
    \begin{pmatrix}
      \sigma_1(y_1) & \ldots & \sigma_1(y_n)\\
    \vdots & \ddots & \vdots\\
    \sigma_n(y_1) & \ldots & \sigma_n(y_n)
    \end{pmatrix}
    \begin{pmatrix}
    c_1\\
    \vdots\\
    c_n
    \end{pmatrix}
    =
    0
  \end{align*}
  means $c_1 = \ldots = c_n = 0$.
\end{proof}

We can now improve the inequality $[E:E^{H}] \geq \abs{H}$ into an equality with some sensible assumptions.

\begin{prop}[]\label{prop:4-9}
  Let $G < \Aut E$ be a finite subgroup. Then
  \begin{align*}
    [E:E^{G}] = \abs{G}
  \end{align*}
\end{prop}
\begin{proof}
  We already gave lower bound, so assume that $[E:E^{G}] > \abs{G}$.

  This means that we could find $b_{1}, \ldots, b_{m} \in E$ linearly independent over $E^{G}$ for some $m > n$.

  If we look at the linear map
  \begin{align*}
    T: E^{m} \to E^{n}, \quad \begin{pmatrix}
    x_1\\
    \vdots\\
    x_m
    \end{pmatrix}
    \mapsto 
    \begin{pmatrix}
      \sigma_1(b_1) & \ldots & \sigma_1(b_m)\\
    \vdots &  & \vdots\\
    \sigma_n(b_1) & \ldots & \sigma_n(b_m)
    \end{pmatrix}
  \end{align*}
  Because $m > n$ we have that $\Ker T \neq 0$ and with $\sigma \sigma_j = \sigma_{s_i}$ it follows that 
  \begin{align*}
    (x_1,\ldots,x_n) \in \Ker T \implies (\sigma(x_1),\ldots,\sigma(x_n)) \in \Ker T
  \end{align*}
  If we set
  \begin{align*}
    r := \min \{k \big\vert v \in \Ker T \setminus 0 \text{ has a $k$-th non-zero coordinate}\}
  \end{align*}
  then let $(x_1,\ldots,x_m) \int \Ker T$ such that it's $r$-th coordinate is non-zero.

  Without loss of generality, we can assume that $x_1 \neq 0$ and we get that for all $\sigma \in G$, by the definition of $r$
  \begin{align*}
    \Ker T \ni
    \frac{1}{\sigma(x_1)} \begin{pmatrix}
      \sigma(x_1)\\
    \vdots\\
    \sigma(x_n)
    \end{pmatrix}
     - \frac{1}{x_1} \begin{pmatrix}
     x_1\\
     \vdots\\
     x_m
     \end{pmatrix}
     =
     \begin{pmatrix}
     0\\
     \sigma(\frac{x_2}{x_1}) - \frac{x_2}{x_1}\\
     \vdots\\
     \sigma(\frac{x_m}{x_1}) - \frac{x_m}{x_1}
     \end{pmatrix}
  \end{align*}
  So when applying $T$ to this vector, we get that
  \begin{align*}
    \sum_{j=1}^{m} \sigma_i(b_j) \frac{x_j}{x_1} = 0
  \end{align*}
  which, when applied to $\sigma_i = \id$ gives us
  \begin{align*}
    \sum_{j=1}^{m} b_j \frac{x_j}{x_1} = 0
  \end{align*}
  This gves us a non-trivial linear combination of $0$, namely
  \begin{align*}
    b_1 + b_2 \frac{x_2}{x_1} + \ldots + b_m + \frac{x_m}{x_1} = 0
  \end{align*}
  which is a contradiction.

  This proves the upper bound and thus $[E: E^{G}] = \abs{G}$
\end{proof}



\begin{cor}[]\label{cor:4-10}
  Let $G,H$ be finite subgroups of $\Aut E$. Then
  \begin{align*}
    E^{G} \subseteq E^{H} \iff H < G
  \end{align*}
\end{cor}
\begin{proof}
  By monotoneity of the fixing fields, $\Leftarrow$ is clear

  Now assume $E^{G} \subseteq E^{H}$ and $H \not\subset G$.
  Then, there exists a $\sigma \in H$ with $\sigma \notin G$.
  But since $\sigma$ fixes every element of $E^{H}$ it also fixes every element from $E^{G}$, and thus $E^{G} = E^{G \cup \{\sigma\}}$.

  With the previous lemma, we get that
  \begin{align*}
    \abs{G} = [E:E^{G}] = [E : E^{G \cup \{\sigma\}}] \geq \abs{G} + 1
  \end{align*}
  which is a contradiction.
\end{proof}
