\section{Dualräume}

\begin{definition}{Dualraum}
    Sei $V$ ein $K$-Vektorraum. Der \textbf{Dualraum} von $V$ ist
    \begin{align*}
        V^{*} := \Hom(V,K)
    \end{align*}
    und besteht aus Linearformen $\phi \in V^*$.
\end{definition}
Von jetzt an $\dim(V) < \infty$.

\begin{satz}{}
    Sei $\mathcal{B} = (v_{1}, \ldots, v_{n})$ eine Basis von $V$. Dann gibt es zu jedem $v_i$ eine eindeutige Linearform $v_i^{*} \in V^{*}$ mit $v_i^{*}(v_j) = \delta_{ij}$.\\
    Die Menge $\mathcal{B}^{*} := (v_1^{*}, \ldots v_n^{*})$ ist eine Basis von $V^{*}$. Insbesondere gilt $\dim(V) = \dim(V^*) \implies V \simeq V^{*}$. (Im unendlichdimensionalen Fall gilt das nicht).
\end{satz}
\textbf{Beispiel:} \quad Für eine Basis $\mathcal{B} = (v_1, \ldots, v_n ) =: A$ als $n \times n$ Matrix ist die Duale Basis wegen der Bedingung $v_i^{*}(v_j) = \delta_{ij}$ gerade die Zeilen von $A^{-1}$.\\

\begin{definition}{Annulator}
    Sei $U \subseteq V$ ein Unterraum von $V$. Dann ist der \textbf{Annulator} von $U$ 
    \begin{align*}
        U^0 := \{\phi \in V^* \big\vert \phi(u) = 0, \forall u \in U\} \subseteq V^*
    \end{align*}
    ein Unterraum von $V^*$ und es gilt $\dim(U^0) = \dim(V) - \dim(U)$, denn ist $(u_{1}, \ldots, u_{k})$ eine Basis von $U$, und $(u_{1}, \ldots, u_{k}, v_{1}, \ldots, v_{r})$ eine Basis von $V$, so ist $(v_{1}^*, \ldots, v_{r}^*)$ eine Basis von $U^0$
\end{definition}

\begin{definition}{Duale von Linearen Abbildungen}
    Seien $F: V \to W$ und $\psi: W \to K \in W^*$ linear. Dann ist die duale Abbildung zu $F$ gegeben durch
    \begin{align*}
        F^*: W^* \to V^* \quad \psi \mapsto F^{*}(\psi) := \psi \circ F \in \Hom(V, K)
    \end{align*}
    \begin{center}
        \begin{tikzcd}[] %\arrow[bend right,swap]{dr}{F}
            V \arrow[]{r}{F} \arrow[swap]{dr}{\psi \circ F} & W \arrow[]{d}{\psi}\\ & \mathbb{K}
        \end{tikzcd}
    \end{center}
    Die resultierende Abbildung gegeben durch
    \begin{align*}
        \Phi: \Hom(V,W) \to \Hom(W^*, V^*) \quad F \mapsto \Phi(F) := F^* 
    \end{align*}
    ist ein Isomorphismus.
\end{definition}

\begin{satz}{}
    Seien $V, W$ zwei $K$-Vektorräume mit Basen $\mathcal{A}, \mathcal{B}$ und $F \in \Hom(V,W)$. Dann ist
    \begin{align*}
        \mathcal{M}_{\mathcal{A}^*}^{\mathcal{B}^*} = \left(\mathcal{M}_{\mathcal{B}}^{\mathcal{A}}\right)^T
    \end{align*}
\end{satz}

\begin{satz}{}
    Sei $F \in \Hom(V,W)$ dann gilt
    \begin{align*}
        Im(F^*) = \left(\Ker(F)\right)^0 \quad \text{und} \quad \Ker(F^*) = \left(Im(F)\right)^0
    \end{align*}
    Insbesondere ist $\rang(F^*) = \rang(F)$ und
    \begin{align*}
        F \text{ injektiv } \implies F^* \text{ surjektiv} \quad \text{und} \quad F \text{ surjektiv } \implies F^* \text{ injektiv}
    \end{align*}
\end{satz}

Für endlich dimensionale Vektorräume gibt einen natürlichen Isomorphismus von $V$ mit seinem Bidualraum $V^{**} := (V^*)^*$ gegeben durch
\begin{align*}
    ev: V \to  V^{**} \quad v \mapsto {ev}_v \in \Hom(V^*, K) \quad ev_v(\phi) = \phi(v)
\end{align*}
Es gelten auch für Basen $\mathcal{B}$ von $V$ und Unterräume $U \subseteq V$
\begin{align*}
    \mathcal{B}^{**} \simeq \mathcal{B} \quad (U^0)^0 \simeq U
\end{align*}

Für ein LGS $Ax = 0$ für $A in M(m\times n,K)$ schreiben wir $a_1, \ldots a_m$ für die Zeilen von $A$. Setzen wir $U := \spn(a_1, \ldots a_n)$, so ist der Lösungsraum des LGS $\mathcal{L} = U^0$. Das dazu duale Problem ist dann:
für gegebenes $W = \spn(w_1, \ldots w_s)$ suche ein $A$, sodass
\begin{align*}
    \mathcal{L} = \{x \big\vert Ax = 0\} = W
\end{align*}
Also suche $U$, sodass $U = W^0$, bzw. $U^0 = W$. Setze $X = (w_1, \ldots w_s) \in M(n\times s,K)$ und löse $X^T a^T = 0$. ($a^T \in (K^n)^*$)




\section{Bilinearformen}

\begin{definition}{Bilinearform}
    Seien $V,W$ $K$-Vektorräume. Eine Abbildung $s: V \times W \to K$ ist eine \textbf{Bilinearform}, wenn für alle $v,v' \in V, w,w' \in W, \lambda \in K$ gilt
    \begin{enumerate}
        \item   $s(v+ \lambda v',w) = s(v,w) + \lambda s(v',w)$
        \item   $s(v, w + \lambda w') = s(v,w) + \lambda s(v,w')$
    \end{enumerate}
    also linear in jedem Argument.
    $s: V \times V \to K$ heisst 
    \begin{itemize}
        \item   \textbf{symmetrisch}, falls $s(v,v') = s(v',v). \forall v \in V$
        \item   \textbf{alternieren}, falls $s(v,v') = -s(v',v), \forall v \in V$
    \end{itemize}
\end{definition}

\begin{definition}{Darstellende Matrix von Bilinearformen}
    Für $V$ mit $\dim V = n < \infty$ ist die \textbf{darstellende Matrix} der Bilinearform $s: V \times V \to K$ bezüglich der Basis $\mathcal{B} = (v_{1}, \ldots, v_{n})$ die eindeutig bestimmte Matrix 
    \begin{align*}
        \mathcal{M}_{\mathcal{B}}(s) := \left(s(v_i,v_j)\right)_{ij} \in M(n\times n,K)
    \end{align*}
    und es gilt für alle $u,v \in V$
    \begin{align*}
        s(u,w) = u^T \mathcal{M}_{\mathcal{B}}(s)w = \sum_{i,j = 1}^{n} s(v_i,v_j)u_iw_j
    \end{align*}
    Die Abbildung $s \mapsto \mathcal{M}_{\mathcal{B}}(s)$ ist bijektiv.
\end{definition}
Es gilt $e_i^T A e_j = a_{ij}$

\begin{satz}{Transformationsformel}
    Sei $s: V \times V \to K$ bilinear. $V$ endlich dimensional mit Basen $\mathcal{A}, \mathcal{B}$. Dann gilt 
    \begin{align*}
        \mathcal{M}_{\mathcal{B}}(s) = \mathcal{M}_{\mathcal{A}}^{\mathcal{B}}(\id)^T \mathcal{M}_{\mathcal{A}}(s) \mathcal{M}_{\mathcal{A}}^{\mathcal{B}}(\id)
    \end{align*}
\end{satz}

\section{Skalarprodukt}

\begin{definition}{Skalarprodukt}
    Eine symmetrische bilinearform $\left<-,-\right>: V \times V \to \R$ heisst \textbf{Skalarprodukt} auf $V$, falls sie zusätzlich positiv definit ist.
    \begin{align*}
        \left<v,v\right> \geq 0, \forall v \in V \quad \text{und} \quad \left<v,v\right> = 0 \Leftrightarrow v = 0
    \end{align*}
    Ein $\R$-Vektorraum $V$ mit einem Skalarprodukt heisst \textbf{euklidischer Raum}.
    Jedes Skalarprodukt induziert eine Norm durch $\Norm{v}:= \sqrt{\left<v,v\right>}$ und der \textbf{Winkel} zwischen zwei Vektoren ist definiert durch
    \begin{align*}
        \cos(\alpha) = \frac{\left<x,y\right>}{\Norm{x} \Norm{y}}
    \end{align*}
\end{definition}

Eine Matrix $A \in M(n\times n,K)$ heisst \textbf{positiv-definit}, falls
\begin{align*}
    v^TAv \geq 0, \forall v \in K^n \quad \text{und} \quad v^TAv = 0 \Leftrightarrow v = 0
\end{align*}
Es gilt $\left<-,-\right>$ pos. definit $\Leftrightarrow \mathcal{M}_{\mathcal{B}}(\left<-,-\right>)$. (Unabhängig von Basis wegen Transformationsformel)

\begin{definition}{Sesquilinearität}
    Sei $V$ ein $\C$-Vektorraum. Eine Abbildung $s: V \times V \to \C$ heisst \textbf{sesquilinear}, falls für alle $v,w,z \in V$ gilt
    \begin{align*}
        s(v + \lambda z,w) = s(v,w) + \lambda s(z,w)\\
        s(v, w + \lambda z) = s(v,w) + \overline{\lambda}s(v,z)
    \end{align*}
    $s$ heisst \textbf{hermitesch}, falls $\forall v,w \in V$ gilt
    \begin{align*}
        s(w,v) = \overline{s(w,v)}
    \end{align*}
\end{definition}

\textbf{Beispiel:} \quad Das Standardskalarprodukt auf $\C^n$ gegeben durch
\begin{align*}
    \left<z,w\right> := \sum_{i = 1}^{n} z_i \overline{w_i}
\end{align*}
ist eine sesquilinearform auf $\C^n$

Wie bei den Bilinearformen gibt es für jede gegebene Basis $\mathcal{B} = (v_{1}, \ldots, v_{n})$ von $V$ mit Sesquilinearform $s$ auf $V$ eine eindeutige Darstellungsmatrix 
\begin{align*}
    \mathcal{M}_{\mathcal{B}}(s) = \left(s(v_i,v_j)\right)_{ij} \in M(n\times n,K)
\end{align*}
und es gilt $s(v,w) = x^T A \overline{y}$, wobei $x,y$ die Koordinaten von $v,w$ bezüglich $\mathcal{B}$ ist. 
Die Transformationsformel zwischen zwei Basen $\mathcal{A}, \mathcal{B}$ von $V$ ist wie folgt
\begin{align*}
    \mathcal{M}_{\mathcal{B}}(s) = {T_{\mathcal{A}}^{\mathcal{B}}}^T \mathcal{M}_{\mathcal{A}} \overline{T_{\mathcal{A}}^{\mathcal{B}}}
\end{align*}

Eine hermitesche Sesquilinearform $s: V \times V \to \C$ heisst \textbf{Skalarprodukt} auf $V$, falls sie noch positiv definit ist.

Ein $\C$-Vektorraum mit Skalarprodukt heisst \textbf{unitärer Raum}.

\begin{satz}{Cauchy-Schwarz'sche Ungleichung}
    Sei $V$ ein euklidischer oder unitärer Vektorraum. Dann gilt für alle $v,w \in V$ die Ungleichung
    \begin{align*}
        \abs{\left<v,w\right>} \leq \Norm{v} \cdot \Norm{w}
    \end{align*}
    und die Gleichheit gilt genau dann, wenn $v$ und $w$ kolinear sind.
\end{satz}

Wir sagen $v,w \in V$ sind \textbf{orthogonal}, falls $\left<v,w\right> = 0$ und schreiben $v \bot w$. Wir sagen zwei Unterräume $U,W \subseteq V$ sind orthogonal, falls $u \bot w, \forall u \in U, w \in W$. Das \textbf{orthogonale Komplement}
\begin{align*}
    U^{\bot} := \{v \in V \big\vert v \bot u, \forall u \in U\}
\end{align*}

Eine Familie von Vektoren $v_{1}, \ldots, v_{n}$ heisst \textbf{orthogonal }, falls $v_i \bot v_j, \forall i\neq j$. Und \textbf{orhtonormal}, falls zusätzlich $\Norm{v_i} = 1$ und es gilt dann $\left<v_i,v_j\right> = \delta_{ij}$.

\begin{satz}{Orthonormalisierungssatz}
    Sei $V$ ein endlichdimensoinaler euklidischer/unitärer Vektorraum und $U \subseteq V$ ein Unterraum mit Orthonormalbasis $(u_{1}, \ldots, u_{k})$. Dann gibt es eine Ergänzung zu einer Orthonormalbasis $(u_{1}, \ldots, u_{k}, v_1, \ldots v_r)$ von $V$. 
\end{satz}
Es gilt für jeden Unterraum $U \subseteq V$ eines eukidischen/unitären Vektorraumes
\begin{align*}
    V = U \oplus U^{\bot} \quad \text{und} \quad \dim V = \dim U + \dim U^{\bot}
\end{align*}

\begin{algorithmus}{Gram-Schmidt}
    Sei $V$ ein euklidischer/unitärer Vektorraum und $W = (w_1, \ldots w_n)$ eine Basis von $V$. Wir konstruieren eine Orthonormalbasis von $V$:
    
    Setze $\hat{v_1} = w_1$ und $v_1 = \frac{\hat{v_1}}{\Norm{\hat{v_1}}}$. Für $j = 2, \ldots n$ setze
    \begin{align*}
        \hat{v_j} = w_j - \sum_{i = 1}^{j-1} \left<v_i,w_j\right>v_i \quad v_j = \frac{\hat{v_j}}{\Norm{\hat{v_j}}}
    \end{align*}
    Dann ist $(v_{1}, \ldots, v_{n})$ eine Orthonormalbasis von $V$.
\end{algorithmus}


\begin{definition}{Orthogonale/ Unitäre Endomorphismen}
    Sei $V$ ein euklidischer/unitärer Vektorraum. $F \in \End(V)$ heisst \textbf{orthogonal/unitär}, falls 
    \begin{align*}
        \forall v,w \in V: \left<F(v),F(w)\right> = \left<v,w\right>
    \end{align*}
\end{definition}
Orthogonale Endomorphismen sind
\begin{enumerate}
    \item	Längenerhaltend: $\Norm{F(v)} = \Norm{v}$
    \item   Winkelerhaltend: $v \bot w \implies F(v) \bot F(w)$
    \item   
    \item   $\lambda \in K$ EW von $F \implies \abs{\lambda} = 1$
\end{enumerate}

\textbf{Lemma:} \quad $\Norm{F(v)} = \Norm{v}, \forall v \in V \implies F$ orthogonal.

\begin{definition}{Orthogonale/Unitäre Matrizen}
    Eine Matrix $A in M(n\times n,K)$ heisst orthogonal,  falls $A^{-1} = A^T$. 
    A heisst unitär, falls $A^{-1} = A^H$
    Schreibe
    \begin{align*}
        \mathcal{O}(n) &:= \{A \in M(n\times n,K) \big\vert A \text{ ist orthogonal}\}\\
        \mathcal{SO}(n) &:= \{A \in \mathcal{O}(n)    \big\vert \det A = 1\}\\
        \mathcal{U}(n) &:= \{A \in M(n\times n,K) \big\vert A \text{ ist unitär}\}
    \end{align*}
\end{definition}

Sei $\mathcal{B}$ eine ONB von einem euklidischen/unitären Vektorraum $V$ und $F \in \End(V)$. Dann gilt
\begin{align*}
    F \text{ ist orthogonal/unitär }  \Leftrightarrow \mathcal{M}_{\mathcal{B}}(F) \in \mathcal{O}(n)/ \mathcal{U}(n)
\end{align*}

Matrizen $A \in \mathcal{O}(2)$ haben alle die Form
\begin{align*}
    A = \underbrace{\begin{pmatrix}
        \cos(\alpha) & -\sin(\alpha)\\
        \sin(\alpha) & \cos(\alpha)
    \end{pmatrix}}_{\text{Rotation um $\alpha$}}
    \quad \text{ oder } \quad A = \underbrace{\begin{pmatrix}
        \cos(\alpha) & \sin(\alpha)\\
        \sin(\alpha) & -\cos(\alpha)
    \end{pmatrix}}_{\text{Spiegelung um die Achse im Winkel $\frac{\alpha}{2}$}}
\end{align*}
für ein $\alpha \in [0,2\pi)$. 


Sei $F \in \End(V)$ orthogonal, $\dim V = n < \infty$. Dann gibt es eine ONB $\mathcal{B}$ von $V$ sodass
\begin{align*}
    \mathcal{M}_{\mathcal{B}}(F) = \begin{pmatrix}
        1_{\ddots_{1}}\\
        & {-1}_{\ddots_{-1}}\\
        & & A_1\\
        & & & \ddots\\
        & & & & A_k
    \end{pmatrix}, \quad A_i \in \mathcal{SO}(2)
\end{align*}
\textbf{Theorem:} \quad Jeder unitärer Endomorphismus besitzt eine ONB aus Eigenvektoren. Insbesondere gibt es für alle $A \in \mathcal{U}(n)$ ein $S \in \mathcal{U}(n)$ sodass 
\begin{align*}
    S^H A S = \begin{pmatrix}
        \pm 1_{\ddots_{\pm 1}}
    \end{pmatrix}
\end{align*}