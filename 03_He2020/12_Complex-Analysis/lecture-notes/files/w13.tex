% ==== 08.12.20 ====
Recall that if $f$ is analytic in $\Omega$, then for $z_0 \in \Omega$ we can write the taylor series
\begin{align*}
	f(z) = f(z_0) + \frac{f'(z_0)}{1!}(z-z_0) + \frac{f^{(2)(z_0)}}{2!}(z-z_0)^{2} + \ldots + \frac{f^{(n)}(z_0)}{n!}(z-z_0)^{n} + f_{n+1}(z)(z-z_0)^{n+1}
\end{align*}
for some analytic function $f_{n+1}$ in $\Omega$. Our goal is to control the $f_{n+1}$. We know that we can write
\begin{align*}
	f_{n+1}(z) = \frac{1}{2\pi i}\int_{C} \frac{f(\xi)}{(\xi - z_0)^{n+1}(\xi - z)}d \xi
\end{align*}
for any circle $C$ such that the corresponding disk $\overline{\Delta}$ is contained in $\Omega$. If $\rho > 0$ is the radius of the circle, then let 
\begin{align*}
	M := \max_{z \in C}\abs{f(z)}
\end{align*}
then we can bound the $f_{n+1}$ by giving upper bounds to the numerator and lower bounds for the denominator
\begin{align*}
		\abs{f_{n+1}(z)(z- z_0)^{n+1}} 
	&\leq
	\abs{z - z_0}^{n+1} \cdot \frac{1}{2\pi} \int_{C} \frac{\abs{f(\xi)}}{\abs{\xi - z_0}^{n+1}\abs{\xi - z}} d \xi\\
	&\leq \abs{z - z_0}^{n+1} \rho \frac{M}{\rho^{n+1} (\rho - \abs{z - z_0}} \\
	&= \rho M \left(
		\frac{\abs{z - z_0}}{\rho}
	\right)^{n+1}
	\frac{1}{\rho - \abs{z - z_0}}
\end{align*}
Then for any $0 < r < \rho$, we take the smaller closed disk.
\begin{align*}
	\overline{\Delta_r} := \left\{z \big\vert \abs{z - z_0} \leq r\right\}
\end{align*}
then inside this closed disk $\overline{\Delta_r}$ we get the stronger bound
\begin{align*}
	\abs{f_{n+1}(z) (z - z_0)^{n+1}} \leq \rho M \left(
		\frac{r}{\rho}
	\right)^{n+1} \frac{1}{\rho - r}
\end{align*}
which shows that the error converges to zero for $n \to \infty$ and since the estimate is the same for all $z \in \overline{\Delta_r}$, the convergence is uniform.

In other words, for any region $\Omega$, and any point $z_0 \in \Omega$, then we can say that the taylor series converges uniformly in the largest disk $\Delta$ contained in $\Omega$.

The issues arise when $z_0$ is close to the boundary. Then the radius of convergence is very small. For example for the function
\begin{align*}
	f(z) = \frac{1}{1 + z^2} \implies \Omega = \C \setminus \{\pm i\}
\end{align*}
over the reals, we can't really see why the taylor series at $z_0 = 0$ only has such a small radius of convergence. But now over the complex numbers we immediately see why the taylor series only converges for $\abs{z} < 1$.

So if $\Omega$ is not circle-shaped, then the taylor series only converges for a small subset of $\Omega$. The key idea is that we take power series of the form
\begin{align*}
	\sum_{n}a_n z^{-n}
\end{align*}
which is called the \textbf{Laurent Series}. This will converge \emph{outside} of a circle for $\abs{z} > R$.

Then we can define a double series of the form
\begin{align*}
	\sum_{n = - \infty}^{\infty} a_n z^{n}
\end{align*}
and we say that this series converges if both the Taylor- and Laurent subseries converge.

And so the double series will converge in an annulus
\begin{align*}
	R_1 < \abs{z} < \abs{R_2}
\end{align*}
in particular the series 
\begin{align*}
	\sum_{n = -\infty}^{\infty}a_n (z - z_0)^{n}
\end{align*}
will converge in some annulus $R_1 < \abs{z - z_0} < R_2$. Moreover we will show that any function $f$ that is analytic in some annulus can be written in terms of this double series
\begin{align*}
	f(z) = \sum_{n = -\infty}^{\infty} A_n(z - z_0)^{n}
\end{align*}
This is particularly useful for $R_1 = 0$ to study functions with isolated singularities.

The Proof idea is to show that if $f(z)$ can be written as the double series, then we can break it down to
\begin{align*}
	f(z) = \sum_{n > 0}A_n (z - z_0)^{n} + \sum_{n \leq 0}A_n(z - z_0)^{n} =: f_1(z) + f_2(z)
\end{align*}
then $f_1$ should be analytic for $\abs{z} < R_2$ and $f_2$ is analytic for $R_1 < \abs{z}$ and then expand each of these in power series.

Then define $f_1$ and $f_2$  as
\begin{align*}
	f_1(z) = \frac{1}{2\pi i} \int_{\abs{\xi - z_0} = r} \frac{f(\xi)}{\xi - z} d \xi\\
	f_2(z) = \frac{-1}{2\pi i} \int_{\abs{\xi - z_0} = r'} \frac{f(\xi)}{\xi - z} d \xi
\end{align*}
which is well defined and analytic for $\abs{z - z_0} < r < R_2$ then do the same for
Notice that the defintion does not depend on $r$ and $r'$

Then we can show using Cauchy's theorem that in the annulus $f(z) = f_1(z) + f_2z)$ because $C_r - C_{r'} \sim 0 \mod \Omega$.

So by Cauchy's Integral formula
\begin{align*}
	f(z) = \frac{1}{2\pi i} \int_{C_r - C_{r'}} \frac{f(\xi)}{\xi - z}d \xi = f_1(z) + f_2(z)
\end{align*}

The next thing we want to look at is to make sense of infinite products
\begin{align*}
	p_{1} \dots p_{n} \dots = \prod_{n = 1}^{\infty}p_n
\end{align*}
to properly define these infinite products we look at the sequence of partial products
\begin{align*}
	P_n = \prod_{k = 1}^{n} p_k
\end{align*}
and we say that the infinite product converges if the sequence $\left(P_{n}\right)_{n = 1}^{\infty})$ of partial products converges.


It is easy to see that for this to converge, we must have
\begin{align*}
	\lim_{n \to \infty} \frac{P_{n+1}}{P_n} = 1 \quad \text{or} \quad \lim_{k \to \infty} p_k = 1
\end{align*}
Usually we will write
\begin{align*}
	\prod_{n= 1}^{\infty} (1 + a_n)
\end{align*}
instead and can understand this product instead through
\begin{align*}
	\sum_{n=1}^{\infty}\log(1 + a_n) 
\end{align*}
so if we write $S_M$ of the $M$-th partial sum of the sum above, then $P_M = e^{S_M}$


We can clearly see that if $S_M$ converges, then so does $P_M$. The converse however is not so clear to see, because it might be that $S_M = 2\pi i M$ is jumping from branch to branch.

\begin{theorem}[]
	The finite product $\prod_{n=1}^{\infty}(1 + a_n)$ with $1 + a_n \neq 0$ converges if and only if $\sum_{n=1}^{\infty}\log(1 + a_n)$ converges.
\end{theorem}
For the proof we can show that
$ \sum \log(1 + a_n)$ converges absolutely if and only if $\sum a_n$ converges absoutely because if $a_n \to 0$, then $n$ large enough
\begin{align*}
	(1 - \epsilon) \abs{a_n} < \abs{\log(1 + a_n)} < (1 + \epsilon) \abs{a_n}
\end{align*}

Which motivates your next definition

\begin{definition}[]
	We say that $\prod_{n = 1}^{\infty}(1 + a_n)$ converges absolutely, if the series $\sum_{n=1}^{\infty}a_n$ converges abosolutely.
\end{definition}


% ==== 11.12.20 ====
\section{Prime Number Theorem}
This section covers material that is not directly relevant for the exam but shows the application of complex analysis in other fields.


Let $\pi(x)$ count the number of primes $p \leq x$.

The prime number theorem says that
\begin{align*}
	\pi(x) \sim \frac{x}{\log x}
\end{align*}
Gauss, Legendre first conjectured this to be the case in the last 1700 and early 1800.

In the mid 1800s, Chebyshev gave lower bounds for $\pi(x)$. He showed that there exist constants $C,D$ such that
\begin{align*}
	C \frac{x}{\log x}\leq \pi(x) \leq D \frac{x}{\log x}
\end{align*}
for $x$ large enough.

And then in the late 1800s, the Prime number theorem was proven by Hadamard and Pussin using complex analysis.

This is striking because when we're counting primes in the natural numers, we don't see any complex numbers or even integrals.


The idea behind the proof is that we know that there always exists a prime between a number and its double. In particular, if we look at the binomial coefficient $\binom{2n}{n}$
we see that that it needs to be divisible by every prime between $n$ and $2n$.
\begin{align*}
	\binom{2n}{n} \geq \prod_{n < p \leq 2n} p \geq n^{\pi(2n) - \pi(n)}
\end{align*}
So we only need to find an upper bound for $\binom{2n}{n}$. We can show that $2^{2n}$ is enough, so if we take the log we get
\begin{align*}
	2n \geq \log_2(n) \left(
		\pi(2n) - \pi(n)
	\right) \implies \pi(2n) - \pi(n) \leq \frac{2n}{\log 2n}
\end{align*}
So if we follow the recursion, we get that
\begin{align*}
	\pi(2n) \leq \frac{2n}{\log_2 n}  + \frac{n}{\log_2 \frac{n}{2}} + \ldots + \frac{\frac{n}{2^{k-1}}}{\log_2 \frac{n}{2^{k}}}
\end{align*}
For $2^{k} = \sqrt{n}$. Then
\begin{align*}
	\pi(2n) \leq \frac{2n}{\log_2 n} + \ldots + \frac{2 \sqrt{n}}{\frac{1}{2} \log n} + \pi(\sqrt{n})
\end{align*}
doing some analysis for this geometric series we obtain
\begin{align*}
	\pi(2n) \leq \frac{8n}{\log_2 n} + \sqrt{n}
\end{align*}
For the lower bound we can also use the binomial formula and refine it.

It turns out that if we can look at a function that is a little nicer than $\pi(x)$. We can define
\begin{align*}
	\theta(x) = \sum_{p \leq x} \log p = \log \prod_{p \leq x} p
\end{align*}
so we show $\theta(x) \sim x$ instead because it is equivalent to the prime number theorem because
\begin{align*}
	\theta(x) = \sum_{p \leq x} \log p \leq \sum_{p \leq x} \log (x) = \pi(x) \log x
\end{align*}
and conversely let $\epsilon > 0$, then
\begin{align*}
	\theta(x) &\geq \sum_{x^{1 - \epsilon} \leq p \leq x} \log p \geq \sum_{x^{1- \epsilon} \leq p \leq x} \log x^{1 - \epsilon}\\
						&= \left(\pi(x) - \pi(x^{1 - \epsilon}\right) (1 - \epsilon) \log x
\end{align*}

The key behind using complex numbers in number theory is the \textbf{Riemann Zeta Function}
\begin{empheq}[box=\bluebase]{align*}
	\zeta(s) = \sum_{n=1}^{\infty} \frac{1}{n^s}
\end{empheq}	

