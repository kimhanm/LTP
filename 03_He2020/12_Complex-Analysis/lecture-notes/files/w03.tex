We saw that differentiability implied the Cauchy-Riemann equations. It seems to be a natural question to ask, wheter the converse is also true and if not, what the weakest possible assumptions are.

\begin{theorem}[]
	If $u(x,y), v(x,y) \in C^1$ i.e. have continuous derivative and satisfy the Cauchy-Riemann equations, then the function $f$ defined as $f(x + iy) := u(x,y) + iv(x,y)$ is analytic.
\end{theorem}
The idea behind the proof is that to write $u,v$ as their first derivative approximation. The continuity implies that the error of the approximation is very small, from which the proof falls out of the equations.\\

\textbf{Proof:} \quad 
\begin{align*}
				u(x + t, y + w) = t \frac{\del u}{\del x} + \frac{\del u}{\del y} + \epsilon_1, \quad \text{where} \quad \text{where} \quad \frac{\epsilon_1}{t + iw} \to 0, \text{ as } t,w \to 0\\
				u(x + t, y + w) = t \frac{\del v}{\del x} + \frac{\del v}{\del y} + \epsilon_2, \quad \text{where} \quad \frac{ \epsilon_2}{t + iw} \to 0, \text{ as } t,w \to 0
\end{align*}
We then can write
\begin{align*}
				f(z + t + iw) - f(z) &= u(x + t,y+w) - u(x,y) + i \left(v(x+t,y+w) - v(x,y)\right)\\
														 &= t \frac{\del u}{\del x} + w \frac{\partial u}{\partial y} + it \frac{\partial v}{\partial x} + iw \frac{\partial v}{\partial y} + \epsilon_1 + \epsilon_2 \\
														 &= t \frac{\partial u}{\partial x} - w \frac{\partial v}{\partial x} + it \frac{\partial v}{\partial w} + iw \frac{\partial u}{\partial x} + \epsilon_1 + \epsilon_2\\
														 &= \left(\frac{\partial u}{\partial x} + i \frac{\partial v}{\partial x}\right)(t + iw) + \epsilon_1 + \epsilon_2
\end{align*}
Which allows us to show, that the following limit exists (i.e $f$ is complex differentiable)
\begin{align*}
				\lim_{t + iw \to 0} \frac{f(z + t + iw) - f(z)}{t + iw} = \frac{\del u}{\del x} + i \frac{\del v}{\del x} + \frac{ \epsilon_1 + i \epsilon_2}{t + iw}
\end{align*}

Now, let's look at some examples of differentiable functions.
$f(z) = 1$ is analytic with derivative $f'(z) = 1$. The identity map $f(z) = z$ is also holomorphic, as
\begin{align*}
				f'(z) = \lim_{h \to 0} \frac{f(z+h) - f(z)}{h} = \lim_{h \to 0} \frac{z + h - h}{h} = 1
\end{align*}
Polynomials $p_n(z) = a_0 + a_1 z + a_2z^2 + \ldots + a_nz^n$ are analytic, and their derivates are as we expect from real analysis.\\
As we are going to show later, we will find that we can approximate every analytic function with polynomials, so we will study them next.\\
A very useful theorem, which we will prove later is the Fundamental theorem of Algebra, which states that every non-constant polynomial has a root in $\C$.\\
This theorem also proves that every polynomial of degree $n \geq 1$ has exactly $n$ roots, if we count roots with their multiplicity.\\
We do this by \emph{factoring out} the roots to get
\begin{align*}
				p(z) = a_n(z - \alpha_1)(z - \alpha_2) \dots (z - \alpha_n)
\end{align*}
We will also see that if we have roots $\alpha_j, \ldots, \alpha_n$ are roots of our polynomial, then there will be a root of its derivative inside the polygon $( \alpha_j, \ldots, \alpha_n)$. This is analogous to the Rolle's theorem in the real case.\\


\begin{ntheorem}[Luca's Theorem]
	If all rots of a polynomial $p(z)$ are in a halfspace $H$, then so are roots of it's derivative $p'(z)$.
\end{ntheorem}
Where we define halfspaces as 
\begin{align*}
	H: \left\{z \in \C \big\vert \text{Im}\frac{z - a}{b} < 0\right\}
\end{align*}

\textbf{Proof:} If decompose the polynomial into its factors (using the fundamental theorem of algebra), we have
\begin{align*}
				p(z) &= a_n(z - \alpha_1)(z - \alpha_2) \dots (z - \alpha_n)\\
				\implies \frac{p'(z)}{p(z)} = \frac{1}{z - \alpha_1} + \frac{1}{z - \alpha_2} + \ldots + \frac{1}{z - \alpha_n}
\end{align*}
Since all roots $\alpha_{1}, \ldots, \alpha_{n} \in H = \left\{z \in \C \big\vert \text{Im}\frac{z-a}{b} < 0\right\}$ we know that for $z \notin H$ we have
\begin{align*}
				\text{Im}\frac{z - \alpha_k}{b} = \text{Im} \frac{z-a}{b} - \text{Im}\frac{\alpha_k - a}{b} >0
\end{align*}
Since inversion changes the sign of the argument (because their arguments should add to zero) we have that $\text{Im}\frac{b}{z - \alpha_k} < 0$, which would imply that if $z$ were a root of the derivate, we wowould have
\begin{align*}
				0 = \text{Im} \left(b \frac{P'(z)}{P(z)}\right) = \sum_{k=1}^{n} \text{Im}\frac{b}{z - \alpha_k} < 0 \lightning
\end{align*}


\subsection{Rational Functions}
The next set of functions we will consider are the \textbf{rational functions}, which are objects of the Form 
\begin{align*}
				R(z) = \frac{P(z)}{Q(z)}, \quad \text{where} \quad P,Q \text{ are polynomials without common factors}
\end{align*}
If $Q(z) \neq 0$, we get the \emph{quotient rule} of the derivative
\begin{align*}
				R'(z) = \frac{P'(z)Q(z) - P(z)Q'(z)}{Q(z)^2}
\end{align*}
But if $Q(z) = 0$ for some $z \in \C$, then $R(z) = \infty$ and we call $z$ a \textbf{pole} of $R$.\\
It would also be useful to find out the \emph{order} of a pole at infinity. And the rational functions allow us to talk a bit easier about the behaviour of the function.\\
For example, if we want to find out $\lim_{z \to \infty} R(z)$ for
\begin{align*}
				R(z) = \frac{a_0 + a_1 z + \ldots + a_nz^n}{b_0 + b_1 z + \ldots + b_mz^m}
\end{align*}
We define the auxiliary function 
\begin{align*}
				\tilde{R}(z) := R(\frac{1}{z}) = \frac{z^m}{z^n} \frac{a_0z^n + a_1z^{n-1} + \ldots + a_n}{b_0z^{m} + b_1 z^{m-1} + \ldots + b_m}
\end{align*}
And instead look at $\lim_{z \to 0} \tilde{R}(z)$. For the case $m > n$, $R(z)$ has a zero of order $m-n$ at infinity. If however $n > m$, then $R(z)$ has a pole	of order $n-m$ at infinity. And if $n = m$, we get $R(\infty) = \frac{a_n}{b_m} \neq 0,\infty$.\\
We can use this knowledge to find out about how rational functions behave outside of infinity. Be counting roots and poles, we know that we have $n$ roots and $m$ poles outside of infinity.\\
But since the difference between $m$ and $n$ is ``accounted for'' at infinity, we know that the number of poles and roots in $\C \setminus \infty$ are the same and equal the maximimum of $n$ and $m$.\\
Since they are the same, we can define the \textbf{order} of a rational Function $R = \frac{P}{Q}$, where $P$ is a polynomial of degree $n$ and $Q$ one of degree $m$ as the number
\begin{align*}
				\text{order}(R) := \max\{n,m\}
\end{align*}

When we consider the special case of a rational function $S(z) = \frac{\alpha z + \beta}{\gamma z + \delta}$ of order $1$, we can sometimes recover its inverse function if $\alpha \gamma - \beta \gamma \neq 0$ to get
\begin{align*}
	z = S^{-1}(w) = \frac{\gamma w - \beta}{-\delta w + \alpha}
\end{align*}
We later also ask ourselves what we can find out about its fixpoints for functions $S(z) = z + a$ or inverse operations $S(z) = \frac{1}{z}$\\

\subsection{Sequences and Series}
In analysis we want to talk about the limits of functions. But for this we need to look at sequences first. If $\left(a_{n}\right)_{n = 1}^{\infty}$ is a sequence in $\C$, we say that $A \in \C$ is the \textbf{limit} of the sequence if
\begin{align*}
\forall \epsilon > 0 \exists N \in \N \quad \text{such that} \quad \forall n \geq N \abs{a_n - A} < \epsilon
\end{align*}

The problem with this definition is that in order to find out whether the limit exists, we need to find the limit first and then argue that the equation above holds.\\
One way to remedy this is to define \emph{Cauchy-Sequence}. A sequence $\left(a_{n}\right)_{n = 1}^{\infty}$ is called a \textbf{Cauchy-sequence}, if
\begin{align*}
\forall > 0 \exists N \in \N \quad \text{such that} \quad \forall n,m \neq N: \abs{a_n - a_m} < \epsilon
\end{align*}
We have proven in real analysis that convergence and being a Cauchy sequence is equivalent.\\
The nice thing here is that we do not need to find the limit $A$ to argue about whether the sequence converges. In particular, if we have two sequences of which we know that the first one converges, if the secnd sequence has $\abs{b_{i+1} - b_i} \leq \abs{a_{i+1} - a_i} \forall i \in \N$, then we know that the second sequence also converges.\\

Now consider the sum of a sequence. We define the partial sums of a sequence $\left(a_{n}\right)_{n = 1}^{\infty}$ to be the sequence $\left(s_{n}\right)_{n = 1}^{\infty}$ given by
\begin{align*}
s_n := a_1 + \ldots + a_n
\end{align*}
Then we can find out that $\left(s_{n}\right)_{n = 1}^{\infty}$ is a Cauchy-sequence if and only if 
\begin{align*}
	\forall \epsilon > 0 \exists N \in \N \quad \text{such that} \quad \forall n \geq N, \forall p \in \N: \abs{a_n + \ldots + a_{n+p}} < \epsilon
\end{align*}
Using the triangle inequality we have that
\begin{align*}
	\abs{a_n + \ldots + a_{n+p}} \leq \abs{a_n} + \ldots + \abs{a_{n+p}}
\end{align*}
So if the sequence of the absolute values $\abs{a_1} + \abs{a_2} + \ldots$ converges, we get something \emph{stronger} than just convergence. We say that $\left(s_{n}\right)_{n = 1}^{\infty}$ is \textbf{absolutely convergent}.


\subsection{Sequences of functions.}
For a sequence of functions $\left(f_{n}(x)\right)_{n = 1}^{\infty}$ we can have multiple notions of convergence.\\
If we look at \textbf{pointwise-convergence}, i.e. when
\begin{align*}
	\forall \epsilon > 0 \forall x, \exists N \quad \text{such that} \quad \forall n > N : \abs{f_n(x) - f(x)} y \epsilon
\end{align*}
The problem with this is that the function can converge at different speeds to the function $f$, for example when the choice of $N$ is dependent on $x$. It can also happen that the pointwise limit of continuous functions can be discontinuous (for example the function $f:[0,1] \to \R, f(x) = x^n$)\\

Therefore the definition of \textbf{uniform convergence} is important:
\begin{align*}
	\forall \epsilon > 0 \exists N \quad \text{such that} \quad \forall x \forall n \geq N: \abs{f_n(x) - f(x)} < \epsilon 
\end{align*} 


\begin{proposition}[]
If $f_n$ is continous for each $n$ and $f_n \to f$ uniformly, then $f$ is continous.
\end{proposition}

Next we can consider a series of functions, i.e. a function given by
\begin{align*}
	g(x) = f_1(x) + \ldots f_n(x) + \ldots
\end{align*}
We say that the $a_1 + a_2 + \ldots + a_n + \ldots$ is a \textbf{majorant} if
\begin{align*}
	\abs{f_n(x)} < M a_n \quad \text{for some } M \in \R\text{ and } n \text{ large enough}
\end{align*}
Then, if $a_1 + \ldots + a_n + \ldots$ converges, then $f_1(x) + \ldots + f_n(x) + \ldots$ is uniformly and absolutely convergent.\\

Next we want to look at \textbf{Power series}, which are objects of the form
\begin{align*}
a_0 + a_1z + a_2z^2 + \ldots + a_n z^n + \ldots
\end{align*}
And of course we can center the polynomial somewhere else and instead write
\begin{align*}
	\sum_{n= 0}^{\infty}a_n (z - z_0)^n
\end{align*}
To better understand them, we can look at the simples interesting example $1 + z + z^2 + \ldots + z^n + \ldots$ which has
\begin{align*}
	 1 + z + \ldots + z^n = \frac{1 - z^n}{1 -z}
\end{align*}
And for $\abs{z} < 1$ the series converges and we have
\begin{align*}
1 + z + z^2 + \ldots + z^n + \ldots = \frac{1}{1-z}
\end{align*}
But for $\abs{z} \geq 1$ this series will diverge. But this isn't really clear. If we chose $z = -1$ or $z = -i$, then it might look like the partial sums will stay bounded, but that isn't enough to fully converge.\\

We will have some nice criteria for the convergence of the sequences

\begin{theorem}[(Abel)]
Let $\sum a_n z^n$ be a power series. There exists a number $0 \leq R \leq \infty$, we call the \textbf{Radius of convergence}, which satisfies
\begin{enumerate}
\item The series converges absolutely for $\abs{z} < R$ and for $\rho < R$ the series converges absolutely uniformly in $\abs{z} < \rho$
\item The series diverges for $\abs{z} > R$
\item If $\abs{z} < R$, then the series is analytic and the derivative is given can be done term-wise and has the same radius of convergence.
\end{enumerate}
\end{theorem}

The idea of the proof is that we calculate
\begin{align*}
	\frac{1}{R} := \limsup_{n \to \infty}\sqrt[n]{\abs{a_n}}
\end{align*}
and do what we did for the simple case $1 + z + z^2 + \ldots + z^n + \ldots$.\\
This theorem is very nice, since it tells us that the places where the power series converges is quite nice. It also allows to build new analytic functions.
